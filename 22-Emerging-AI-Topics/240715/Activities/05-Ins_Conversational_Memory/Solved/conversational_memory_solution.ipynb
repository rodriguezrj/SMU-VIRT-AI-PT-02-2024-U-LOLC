{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables.\n",
    "load_dotenv()\n",
    "\n",
    "# Set the model name for our LLMs.\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "# Store the API key in a variable.\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two common breeds of cats are the domestic shorthair and the Siamese.\n",
      "\n",
      "Poodles, Bichon Frises, and Portuguese Water Dogs are known to shed the least among the breeds listed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model.\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"What are two common breeds of cats?\"\n",
    "\n",
    "# Pass the query to the invoke method, and print the result.\n",
    "result = llm.invoke(query)\n",
    "print(result.content)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those shed the least?\"\n",
    "\n",
    "# Pass the query to the invoke method, and print the result.\n",
    "result = llm.invoke(query)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for conversational memory.\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two common breeds of cats are the domestic shorthair and the Persian. The domestic shorthair is a mixed breed cat with a short coat and a variety of colors and patterns. The Persian is known for its long, luxurious coat and flat face. Both breeds are popular choices for pet owners around the world.\n",
      "\n",
      "The Persian breed tends to shed less than the domestic shorthair due to its long coat that is less prone to shedding. However, it's important to note that all cats shed to some extent, so regular grooming is still necessary to minimize shedding.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model.\n",
    "llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "# Initialize an object for conversational memory.\n",
    "buffer = ConversationBufferMemory()\n",
    "\n",
    "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
    "conversation = ConversationChain(llm=llm, verbose=False, memory=buffer)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"What are two common breeds of cats?\"\n",
    "\n",
    "# Pass the query to the predict method and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those shed the least?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: what is a dog\n",
      "Assistant: A dog is a domesticated mammal that belongs to the Canidae family. They are known for their loyalty, companionship, and diverse breeds with different characteristics. Dogs have been bred for various purposes, such as hunting, herding, guarding, and companionship. They have a keen sense of smell, hearing, and are known for their ability to learn and follow commands. Dogs come in different sizes, shapes, and colors, and are popular pets worldwide.\n",
      "User: What did you just tell me?\n",
      "Assistant: I just provided you with information about what a dog is - a domesticated mammal known for its loyalty, companionship, and diverse breeds with different characteristics. Dogs have been bred for various purposes and have keen senses of smell and hearing. They come in different sizes, shapes, and colors and are popular pets worldwide.\n",
      "User: q\n"
     ]
    }
   ],
   "source": [
    "buffer = ConversationBufferMemory()\n",
    "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
    "conversation = ConversationChain(llm=llm, verbose=False, memory=buffer)\n",
    "while True:\n",
    "    query = input(\"User: \")\n",
    "    print()\n",
    "    if query == \"q\":\n",
    "        print(\"See ya\")\n",
    "        break\n",
    "    print()\n",
    "    print(\"Assistant:\", conversation.predict(input=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summaries as memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for summary-based memory.\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What are two common breeds of cats?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Two common breeds of cats are the domestic shorthair and the Persian. Domestic shorthair cats are mixed breed cats with short hair, while Persians are known for their long, luxurious coats and flat faces.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks the AI to name two common breeds of cats. The AI responds that the domestic shorthair and Persian are common breeds. Domestic shorthair cats are mixed breed with short hair, while Persians are known for their long coats and flat faces.\n",
      "Human: Which of those shed the least?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Persian cats tend to shed less than domestic shorthair cats because of their long, thick coats. However, regular grooming is still necessary to prevent matting and keep their fur in good condition.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks the AI to name two common breeds of cats. The AI responds that the domestic shorthair and Persian are common breeds, with domestic shorthairs being mixed breed with short hair and Persians known for their long coats and flat faces. Persian cats tend to shed less than domestic shorthair cats due to their long, thick coats, but regular grooming is still necessary.\n",
      "Human: Which one tends to weigh more?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Persian cats tend to weigh more on average compared to domestic shorthair cats. Adult Persian cats can weigh anywhere from 7 to 12 pounds, while domestic shorthair cats typically weigh between 8 to 12 pounds. However, individual cat weights can vary depending on factors such as age, diet, and activity level.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for summaries.\n",
    "summary_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                         model_name=OPENAI_MODEL,\n",
    "                         temperature=0.0)\n",
    "\n",
    "# Initialize the model for output.\n",
    "chat_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                      model_name=OPENAI_MODEL,\n",
    "                      temperature=0.7)\n",
    "\n",
    "# Initialize an object for conversational memory.\n",
    "buffer = ConversationSummaryMemory(llm=summary_llm)\n",
    "\n",
    "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
    "conversation = ConversationChain(llm=chat_llm, memory=buffer, verbose=True)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"What are two common breeds of cats?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those shed the least?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which one tends to weigh more?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity-based Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for entity-based memory.\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Godric': '', 'Maine Coon': '', 'Luna': '', 'Bombay': ''}\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Last line:\n",
      "Human: There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{'Godric': 'Godric is a Maine Coon.', 'Luna': 'Luna is a Bombay cat.'}\n",
      "\n",
      "Current conversation:\n",
      "Human: There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\n",
      "AI: That's great to hear that there are two cats up for adoption! Godric being a Maine Coon and Luna being a Bombay sound like adorable cats. Maine Coons are known for their large size, friendly demeanor, and tufted ears, while Bombays are known for their sleek black coat and playful personality. It's wonderful that these cats are looking for a loving home. Do you have any specific questions about Godric, Luna, or anything else related to cat adoption?\n",
      "Last line:\n",
      "Human: Which of those cats probably has shorter fur?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Luna, the Bombay cat, probably has shorter fur compared to Godric, the Maine Coon. Bombays are known for their sleek, short coat, while Maine Coons have a longer, thicker coat with a prominent ruff around their neck. If you prefer a cat with shorter fur that requires less grooming, Luna might be the better choice for you. Is there anything else you would like to know about Godric, Luna, or cat adoption in general?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an assistant to a human, powered by a large language model trained by OpenAI.\n",
      "\n",
      "You are designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, you are able to generate human-like text based on the input you receive, allowing you to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. You have access to some personalized information provided by the human in the Context section below. Additionally, you are able to generate your own text based on the input you receive, allowing you to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether the human needs help with a specific question or just wants to have a conversation about a particular topic, you are here to assist.\n",
      "\n",
      "Context:\n",
      "{}\n",
      "\n",
      "Current conversation:\n",
      "Human: There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\n",
      "AI: That's great to hear that there are two cats up for adoption! Godric being a Maine Coon and Luna being a Bombay sound like adorable cats. Maine Coons are known for their large size, friendly demeanor, and tufted ears, while Bombays are known for their sleek black coat and playful personality. It's wonderful that these cats are looking for a loving home. Do you have any specific questions about Godric, Luna, or anything else related to cat adoption?\n",
      "Human: Which of those cats probably has shorter fur?\n",
      "AI: Luna, the Bombay cat, probably has shorter fur compared to Godric, the Maine Coon. Bombays are known for their sleek, short coat, while Maine Coons have a longer, thicker coat with a prominent ruff around their neck. If you prefer a cat with shorter fur that requires less grooming, Luna might be the better choice for you. Is there anything else you would like to know about Godric, Luna, or cat adoption in general?\n",
      "Last line:\n",
      "Human: Which cat probably has a calmer temperament?\n",
      "You:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "In general, Maine Coons like Godric are known for their friendly and easygoing temperament. They are often described as gentle giants and are typically good-natured and sociable cats. On the other hand, Bombays like Luna are known for their playful and affectionate personalities. While both cats can be calm and loving, Maine Coons are often seen as more laid-back and relaxed compared to the energetic and curious nature of Bombays. Ultimately, individual personalities can vary, so it's important to spend time with both cats to see which one best fits your lifestyle and preferences. Do you have any other questions about Godric, Luna, or their temperaments?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for entities.\n",
    "entity_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                        model_name=OPENAI_MODEL,\n",
    "                        temperature=0.0)\n",
    "\n",
    "# Initialize the model for output.\n",
    "chat_llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY,\n",
    "                      model_name=OPENAI_MODEL,\n",
    "                      temperature=0.7)\n",
    "\n",
    "# Initialize an object for conversational memory.\n",
    "buffer = ConversationEntityMemory(llm=entity_llm)\n",
    "\n",
    "# Create the chain for conversation, using a ConversationBufferMemory object.\n",
    "conversation = ConversationChain(\n",
    "    llm=chat_llm, \n",
    "    memory=buffer, \n",
    "    verbose=True, \n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    ")\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"There are two cats up for adoption: Godric is a Maine Coon, and Luna is a Bombay.\"\n",
    "\n",
    "# Pass the query to the predict method.\n",
    "result = conversation.predict(input=query)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which of those cats probably has shorter fur?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "# Define a query as a string.\n",
    "query = \"Which cat probably has a calmer temperament?\"\n",
    "\n",
    "# Pass the query to the predict method, and print the result.\n",
    "result = conversation.predict(input=query)\n",
    "print(result)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
